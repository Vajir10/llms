{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91095f88-600e-4620-8ef9-20cbbd899c0a",
   "metadata": {},
   "source": [
    "#### Content:\n",
    "- preparing dataset from Hub\n",
    "- how to use high-level trainer API to fine-tune a model\n",
    "- create custom training loop\n",
    "- how to leverage HuggingFace accelerate lib to easily run custom training loop on any distributed setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fcc4493-5341-4a6d-b7e7-a23fbc84547a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## LOAFING DATA FROM THE HUB -> MRPC DATASET\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset('glue', 'mrpc') # return dataset dictionary for training , validation and test\n",
    "# it downloads and caches the dataset in ~/.cache/huggingface/datasets, we can change the cache dir, by changing env variable\n",
    "# HF_HOME to new dir\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2dd23c7-3741-4fd6-ab30-76a62d7cd056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accessing training data from dataset dict\n",
    "raw_train_dataset = raw_datasets['train']\n",
    "raw_train_dataset[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c633c335-cb59-481b-a2a0-7d2e6adb625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None), 'idx': Value(dtype='int32', id=None)}\n"
     ]
    }
   ],
   "source": [
    "# since label is already integer, we dont need to do any preprocessing on the labe;\n",
    "# checking which integer belongs to which class, use feature properties of dataset\n",
    "print(raw_train_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9eff60a-d313-411a-9080-a2b7c1045b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 102, 2023, 2003, 1996, 2117, 6251, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the raw text into tokens\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\" # will use this model arch\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# tokenizer takes either single string or list of string\n",
    "# or it also can take up pait of string,  \n",
    "\n",
    "inputs = tokenizer('This is the first sentence', 'This is the second sentence')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731a9b33-2368-4f11-bbf6-ed97dc0665fa",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "toke_type_ids correponds to sentence index,\n",
    "bert tokenizer also adds extra token at starting and ending of the sentence, since bert model expect that tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98cb86b1-30d7-44f4-92a9-b8caae691592",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '[SEP]', 'this', 'is', 'the', 'second', 'sentence', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e9a5935-1e78-4602-b228-4f5c2ccea227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use tokenizer to convert all the dataset in a single step,\n",
    "\n",
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets['train']['sentence1'], # list of first sentence\n",
    "    raw_datasets['train']['sentence2'], # list of 2nd sentence\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "# but this method has disadvantage that, it will try to load complete train data on RAM, so if there is not enough\n",
    "# RAM avialable this will not work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f18ca20d-bcba-4498-81b8-3d2660e2cd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3668/3668 [00:00<00:00, 8178.91 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Hugging face lib, it uses APache Arrow files stored on disk, and it only keep the samles hat we ask\n",
    "\n",
    "# to keep the data in dataset format, we can use map method, to do some preprocessing required for each sentence\n",
    "\n",
    "# this function take input a dictioanry, and return a new dictionary with new keys and old key(updated dict)\n",
    "# it can also work if example['sentence'] has list of dictioanry\n",
    "def tokenize_example(example):\n",
    "    return tokenizer(example['sentence1'], example['sentence2'], truncation=True)\n",
    "\n",
    "\n",
    "# batched=True is applued to make the processing faster, and in a batch manner\n",
    "tokenized_dataset = raw_datasets.map(tokenize_example, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ac01303-6d10-49fa-974f-fc7d1f025fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using dynamic padding through collate function, which concate sample inside the dataloader\n",
    "## using huggingface collator fucntion\n",
    "#collate function. It’s an argument you can pass when you build a DataLoader, \n",
    "#the default being a function that will just convert your samples to PyTorch tensors \n",
    "#and concatenate them (recursively if your elements are lists, tuples, or dictionaries).\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d864eed2-60f3-44dc-9094-d847e4467dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking data collator working\n",
    "samples = tokenized_dataset['train'][:8]\n",
    "samples = {k:v for k, v in samples.items() if k not in ['idx', 'sentence1', 'sentence2']}\n",
    "# getting lenght of each input ids\n",
    "[len(x) for x in samples['input_ids']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5218ae-3d43-43e9-957f-f4ea56fb1f20",
   "metadata": {},
   "source": [
    "each sample has different lenght, use the collator function to convert into same length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec7e50ac-5925-4666-822f-e1d0de48475f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425015cd-f046-4ff7-8b1b-c6287483f4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afb844e-65dc-404e-b2a5-8cafe5aaf7da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b65e4228-92f0-46dc-881e-ec1bf2898ecd",
   "metadata": {},
   "source": [
    "##### Defining Training Arguments\n",
    "- it contain all the hyperparameter that trainer use for training and evaluation\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c22b4e43-0522-4e61-a087-2c0afc62ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments('test-trainer') # only giving dir, others default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1dcb502-7c31-434f-b32c-cf3bb0305013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# defining a model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "# since bert is not trained for classification, Head will be discarded for above method\n",
    "# and new head will be added with random weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84465714-b1aa-4bca-aeae-f33c3d88a40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e9f707-9944-4d88-8808-53e9ae9f9ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36fedb7-3d67-4946-bb33-b889f8911fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
