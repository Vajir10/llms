{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369c19b5-6fc7-44c5-ab45-08be9ffa02b5",
   "metadata": {},
   "source": [
    "### LLMs (Large Language Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf3bace-ffe8-4086-9acc-946b0299ccda",
   "metadata": {},
   "source": [
    "- Characterization of LLMs:\n",
    "    - Scale: contain millions, billions or even more parameters\n",
    "    - General capabilities: it can perform multiple taks without task-specific training\n",
    "    - In-context learning: it can learn from examples provided in prompt\n",
    "    - Emergent abilities: As these models frow in size, it can demonstrate capabilities thayt werenot explicitily programmed for\n",
    "\n",
    "- Limitations of LLM:\n",
    "    - Hallucinations: can generate incorrect information confidently.\n",
    "    - Lack of true understanding: lack of understanding of the world and operate purely on statistical patterns.\n",
    "    - Biases: may reproduce biases present in training data or inputs\n",
    "    - Context Windows: have limited context windows\n",
    "    - Computational resources: require significant computational resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "185feaf0-b159-4a71-bbca-300ac6451cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vajir/anaconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## Tranformer library, pipeline object\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9330ab3c-476a-4964-8eeb-22b880f6cfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "classifier_object = pipeline('sentiment-analysis')\n",
    "# by default, pipeline function slect particular pre-trained model that has been fine-tuned for sentiment analysis\n",
    "# model wgets downloaded, and cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa62c8db-ddfc-46ca-a073-9c902208a389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598050713539124}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 3 MAJOR STEPS OF PIPELINE:\n",
    "# 1. the text is preprocessed intop a format the model can understand\n",
    "# 2. preprocessed input are passed to the model\n",
    "# 3. prections of the model are post processed , to understanble langauge\n",
    "\n",
    "# passing string for analysis\n",
    "classifier_object(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae59bf9e-2bf7-455b-b698-78e43097f723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598050713539124},\n",
       " {'label': 'POSITIVE', 'score': 0.996126115322113},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# passing list of sentence to get predictions\n",
    "inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I don't hate this so much!\",\n",
    "    \"I hate this so much!\"\n",
    "]\n",
    "classifier_object(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a973650f-8355-4f97-a5d5-6b1090f7e26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "005184eb-e629-41e6-adaa-aa8aecbb4f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library on huggingface',\n",
       " 'labels': ['education', 'sports', 'politics'],\n",
       " 'scores': [0.9443621039390564, 0.03222975879907608, 0.02340812422335148]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ZERO SHOT CLASSIFICATION\n",
    "## classify text without prior training on specific labels\n",
    "##allow us to specify which labels to use for classification\n",
    "\n",
    "classifier_object = pipeline('zero-shot-classification')\n",
    "\n",
    "\n",
    "classifier_object(\n",
    "    'This is a course about the Transformers library on huggingface',\n",
    "    candidate_labels = ['education', 'politics', 'sports'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e833e1-3f64-4e5c-ac68-392b8e019197",
   "metadata": {},
   "source": [
    "##### Hallucination experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8ae6aad-00ab-4724-b61b-df4f6d45e9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library on huggingface',\n",
       " 'labels': ['educion', 'sports', 'politics'],\n",
       " 'scores': [0.9692893624305725, 0.017789985984563828, 0.012920672073960304]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mis spelling the label, still model is able to assign high classes\n",
    "classifier_object(\n",
    "    'This is a course about the Transformers library on huggingface',\n",
    "    candidate_labels = ['educion', 'politics', 'sports'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0e8510d-e1c3-42ba-87f2-efa5fdb705bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library on huggingface',\n",
       " 'labels': ['educion', 'educations', 'sports', 'politics'],\n",
       " 'scores': [0.7425496578216553,\n",
       "  0.23392359912395477,\n",
       "  0.013628487475216389,\n",
       "  0.009898222051560879]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding both label, correct and incorrect, mis-spelled label got more probability\n",
    "classifier_object(\n",
    "    'This is a course about the Transformers library on huggingface',\n",
    "    candidate_labels = ['educations', 'educion', 'politics', 'sports'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a82fa9d-790d-4338-a733-a3dc13a39bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40bfe91c-f217-4c72-9d81-32f986beb08b",
   "metadata": {},
   "source": [
    "##### Text Generation\n",
    "- given some text(prompt), model tries to complete the text, and it stops based on end of statement logic\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0e5e3bc-54cf-4dd3-b2c4-b5ee4325298c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"In this course, we will teach you how to get started with the Arduino development environmentâ€¦ You won't need the tools needed to make an accurate, reliable and functional digital representation of your Arduino, but you will use them to design, create, produce\"}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba1d27e6-1dfc-45f9-8e13-25cf95e86109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to use the Python programming language to solve real-world problems. We will start by introducing you to the basics of Python, including its syntax, data types'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define model in pipeline method\n",
    "# control total lenght of the generated text, and num of different sequences returned by model\n",
    "generator = pipeline('text-generation', model='HuggingFaceTB/SmolLM2-360M')\n",
    "generator(\"In this course, we will teach you how to\", max_length=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2175e52-8a85-4e36-b46e-6a64d40e06d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c4fad9-51d7-4e43-bd4c-3523331bc727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e872a22d-b966-4331-b405-a69419a4d5a7",
   "metadata": {},
   "source": [
    "##### Mask filling\n",
    " - model try to fill the blanks given a text\n",
    " - topk arguments control how many different outcomes we want\n",
    " - special token \"(< mask >)\" where it try to predict the missing word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc57e731-73ef-4c72-b28d-14b5e5103e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.19631582498550415,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical models'},\n",
       " {'score': 0.044492267072200775,\n",
       "  'token': 745,\n",
       "  'token_str': ' building',\n",
       "  'sequence': 'This course will teach you all about building models'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unmasker = pipeline('fill-mask')\n",
    "unmasker('This course will teach you all about <mask> models', top_k=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a1806e-bbc7-4cd9-9d8f-820dbea9cfcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ded4cecf-c4af-4a4c-83a5-fa0525a6d84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.9983753),\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.99058795),\n",
       "  'word': 'Face',\n",
       "  'start': 33,\n",
       "  'end': 37},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.99158233),\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 41,\n",
       "  'end': 49}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Named Entity recognition\n",
    "# ner = pipeline('ner', grouped_entities=True)\n",
    "ner(\"My name is Sylvain and I work at Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2ccb9-d59e-4cec-8836-982c1013dff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c7b5d11-69d7-4a8e-a306-3301de0f1ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.5335550308227539, 'start': 33, 'end': 45, 'answer': 'hugging face'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Question answering given a context\n",
    "\n",
    "qna = pipeline(\"question-answering\")\n",
    "qna(question='where do i work?', context='my name is sylvain and i work at hugging face in brooklyn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a286586e-90b2-459a-94a6-84ba4b14740b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b4bba-b2b4-44c3-b651-6db1e6b6e7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5677bc22-9f00-415e-a389-7acd174063c2",
   "metadata": {},
   "source": [
    "#### bit on how transformer works\n",
    "- all llms goes through pre-training process, using self-supervised method\n",
    "- self supervise method: method where objective is automatically computed from the inputs of the model(no explicit label needed)\n",
    "- it devlops statistical understanding of the langauage it has been trained on, not suitable for specific task yet\n",
    "- after pre-training, generally fine-tun is done on specific task in supervised way\n",
    "- two types of pre-training can be done on text data:\n",
    "    - causal language modeling: model try to predict next word, given all previous word\n",
    "    - masked langauge modeling: try to predict the masked word given all words in sentence\n",
    "- Model has 2 componenet:\n",
    "    - Encoder: encoder recieves inputs and calculate feature in lower dims\n",
    "    - Decoder: uses encoder representation along with other input to generate target sequence\n",
    "- Each part can be used independently, based on the what type of task we are tring to solve:\n",
    "    - Encoder only models: Good for tasks that requires understanding of the input, e.g: sentence classification, and ner\n",
    "    - Decoder only models: Good for generative task, e.g: text generation\n",
    "    - Encoder-Decoder models: good for generative task that require input, such as translation or summarization\n",
    "- Attention concept\n",
    "    - a word by itself has meaning, but the meaning is deeply affected by the context\n",
    "    - it was devloped for langauge translation\n",
    "    - in the encoder, the attention layer can use all words in sentence(since we require all the words in sentence to understand the whole meaning)\n",
    "    - the decoder howeever predict words in sequential manner one by one, it takes all the previous words that was generated by decoder, and representation of inputs from encoder?\n",
    "\n",
    "\n",
    "- Context lenght & Attention span:\n",
    "    - it refers to the maximum number of tokens that the LLM can process at once\n",
    "    - it depends on several factors:\n",
    "        - model architecture and size\n",
    "        - Available computational resources\n",
    "        - The complexity of the input and desired output\n",
    "- Prompting:\n",
    "    - when we pass information to LLMs, structure input in a way which guides the generation of the LLMs to the desired output\n",
    "    - Since model primary task is to predict next token, it is essential to craft better prompts\n",
    "\n",
    "\n",
    "\n",
    "- Inference of LLMs can be devided into two phase:\n",
    "    - The prefill phase:\n",
    "        - Tokenization: converting input text into tokens\n",
    "        - EMbedding conversion: transforming these tokens into numerical representation that captures its meaning\n",
    "        - Initial Processing: running these embedding through model's nn to create representation of the all tokens and its context\n",
    "\n",
    " \n",
    "    - The Decode phase:\n",
    "         - Attentions computation: Looking back at all previous tokens to understand context\n",
    "         - Probability calculations: determining the lieklihood of each possible next token\n",
    "         - Token selection: choosing the next token based on these probabilities\n",
    "         - continuation check: Deciding whether to continue or stop generation\n",
    "\n",
    "\n",
    "\n",
    "##### Sampling Strategies\n",
    "- Understanding token selections: From Probabilities to token choices\n",
    "    - Raw Logits: raw output from model, without any post processing\n",
    "    - Tempreture control: like a creativity dial, higher settings (>1.0) makes choices more random and creative, lower settings(<1.0) make it more deterministic\n",
    "    - Top-p(Nucleus) Sampling: Instead of considering all possible words, we only look at the most likely ones that add up to choosen probability threshold\n",
    "    - Top-k Filtering: An alternative approach where we only consider the k most likely next word\n",
    "\n",
    "- Managing Repetition: Keeping output fresh:\n",
    "    - Presence Penalties: A fixed penalty applied to any token that has appered before, regardless of how often. helps prevent the model from reusing same words\n",
    "    - Frequency Penalty: A scaling penalty that increases based on how often a token has been used. The more a word appears, the less likely it is to be choosen again\n",
    "\n",
    "- Controlling Generation Length:\n",
    "    - Token limits: setting minimum and maximum token counts\n",
    "    - Stop Sequences: Defining specific patterns that signal the end of generation\n",
    "    - End of sequence detection: Letting the model naturally conclude its reponse\n",
    "\n",
    "\n",
    "- Beam Search\n",
    "    - Instead of committing to a single choice at each step, it explores multiple possible paths simulaneoulsy - like a chess player thinking several moves ahead\n",
    "    - Steps:\n",
    "        - At each step, maintain multiple candidate sequences(5-10)\n",
    "        - FOr each candidate, compute probabilities for the next token\n",
    "        - Keep only the most pronimising combinations of sequences and next tokens\n",
    "        - Continue this process untill reaching the desired lenght or stop conditions\n",
    "        - Select the sequence with the highest overall probability\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Practical Challenge and optimization:\n",
    "    - Time to First Token(TTFT): How quickly can we get the first response? It is mostly affected by prefill phase\n",
    "    - Time Per Output Token(TPOT): How fast can we generate susequent tokens? overall generation speed\n",
    "    - Throughput: How many request can we handle simulateneously, affects scaling and cost efficiency\n",
    "    - VRAM usage: How much GPU memory do we need?\n",
    "    - Context Length challenge:\n",
    "        - Memory usage: Grows quadratically with context length\n",
    "        - Processing SPeed: Decrease linearly with longer contexts\n",
    "        - Resource Allocation: Require careful balancing of VRAM usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1232ad5-817b-4ca6-87b0-02e02eef6c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6135191f-55dd-4b3b-b764-63636b586d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
